{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposals User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# API URL for fetching user data\n",
    "user_api_url = \"https://forum.arbitrum.foundation/c/proposals/7.json\"\n",
    "# Specify CSV file path for user data\n",
    "user_csv_file_path = \"proposals_users_data.csv\"\n",
    "\n",
    "def fetch_users(api_url, page):\n",
    "    # Make request to fetch user data for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract user data from the response\n",
    "    users = data[\"users\"] if \"users\" in data else []\n",
    "    return users\n",
    "\n",
    "# Set to store processed user IDs\n",
    "processed_user_ids = set()\n",
    "\n",
    "# Write user data to CSV file\n",
    "with open(user_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as user_csv_file:\n",
    "    # Create CSV writer for user data\n",
    "    user_csv_writer = csv.writer(user_csv_file)\n",
    "\n",
    "    # Write user data header\n",
    "    user_header = [\"User Id\", \"Username\", \"Name\", \"Avtar Template\", \"Moderator\", \"Trust Level\"]\n",
    "\n",
    "    user_csv_writer.writerow(user_header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write user data until there are no more pages\n",
    "    while True:\n",
    "        # Fetch user data for the current page\n",
    "        users = fetch_users(user_api_url, page_num)\n",
    "\n",
    "        # Break the loop if no users are returned\n",
    "        if not users:\n",
    "            break\n",
    "\n",
    "        # Write user details, avoiding duplicates\n",
    "        for user_data in users:\n",
    "            user_id = user_data[\"id\"]\n",
    "\n",
    "            # Check if the user ID has already been processed\n",
    "            if user_id not in processed_user_ids:\n",
    "                # Add the user ID to the set of processed IDs\n",
    "                processed_user_ids.add(user_id)\n",
    "\n",
    "                # Initialize user data list with default values\n",
    "                user_row = [\n",
    "                    user_id,\n",
    "                    user_data[\"username\"],\n",
    "                    user_data[\"name\"],\n",
    "                    user_data[\"avatar_template\"],\n",
    "                    user_data.get(\"moderator\", False),\n",
    "                    user_data[\"trust_level\"],\n",
    "                ]\n",
    "\n",
    "                user_csv_writer.writerow(user_row)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"User data has been written to {user_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposals Topic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://forum.arbitrum.foundation/c/proposals/7.json\"\n",
    "\n",
    "# Specify CSV file path\n",
    "csv_file_path = \"proposals_topics_data.csv\"\n",
    "\n",
    "def fetch_topics(api_url, page):\n",
    "    # Make request for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract topics from the current page\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    return topics\n",
    "\n",
    "# Function to format date and time\n",
    "def format_datetime(datetime_str):\n",
    "    # Convert the string to a datetime object\n",
    "    dt_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    # Format the datetime object as a string in the desired format\n",
    "    return dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    # Create CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write header\n",
    "    header = [\"Topic ID\", \"Title\", \"Fancy Title\", \"Slug\", \"Posts Count\", \"Reply Count\", \"Highest Post Number\",\n",
    "              \"Image URL\", \"Created At\", \"Last Posted At\", \"Views\", \"Like Count\", \"Pinned\", \"Unpinned\", \"Closed\", \"Visible\",\n",
    "              \"Tags\", \"Last Poster Username\", \"Category ID\", \"posters\", \"Original Poster ID\"]\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write topics until there are no more topics\n",
    "    while True:\n",
    "        topics = fetch_topics(api_url, page_num)\n",
    "\n",
    "        # Break the loop if no topics are returned\n",
    "        if not topics:\n",
    "            break\n",
    "\n",
    "        # Write topic details\n",
    "        for topic in topics:\n",
    "            # Extract the user IDs of the posters, excluding the original poster\n",
    "            user_ids_posters = [poster[\"user_id\"] for poster in topic[\"posters\"] if poster[\"user_id\"] != topic[\"posters\"][0][\"user_id\"]]\n",
    "\n",
    "            # Initialize data list with default values\n",
    "            data = [\n",
    "                topic[\"id\"],\n",
    "                topic[\"title\"],\n",
    "                topic[\"fancy_title\"],\n",
    "                topic[\"slug\"],\n",
    "                topic[\"posts_count\"],\n",
    "                topic[\"reply_count\"],\n",
    "                topic[\"highest_post_number\"],\n",
    "                topic[\"image_url\"],\n",
    "                format_datetime(topic[\"created_at\"]),  # Format Created At\n",
    "                format_datetime(topic[\"last_posted_at\"]),  # Format Last Posted At\n",
    "                topic[\"views\"],\n",
    "                topic[\"like_count\"],\n",
    "                topic[\"pinned\"],\n",
    "                topic[\"unpinned\"],\n",
    "                topic[\"closed\"],\n",
    "                topic[\"visible\"],\n",
    "                topic[\"tags\"],\n",
    "                topic[\"last_poster_username\"],\n",
    "                topic[\"category_id\"],\n",
    "                topic[\"posters\"],\n",
    "                topic[\"posters\"][0][\"user_id\"] if topic[\"posters\"] else None,\n",
    "            ]\n",
    "\n",
    "            csv_writer.writerow(data)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposals Post Data (Post available on each topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch and store data from a specific API endpoint\n",
    "def fetch_and_store_data(api_url, post_number):\n",
    "    url = f\"{api_url}{post_number}.json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"post_stream\" in data and \"posts\" in data[\"post_stream\"]:\n",
    "            posts = data[\"post_stream\"][\"posts\"]\n",
    "\n",
    "            # Remove duplicates based on post_number\n",
    "            posts = [post for post in posts if post[\"post_number\"] > post_number]\n",
    "\n",
    "            if posts:\n",
    "                # Write data to CSV file\n",
    "                with open(\"proposals_posts_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    fieldnames = [\"Topic ID\", \"Username\", \"Post Created At\", \"Post Description\", \"Post Number\"]\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    # Write headers if the file is empty\n",
    "                    if csv_file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    # Write posts data\n",
    "                    for post in posts:\n",
    "                        # Use BeautifulSoup to extract text from HTML\n",
    "                        soup = BeautifulSoup(post[\"cooked\"], \"html.parser\")\n",
    "                        cleaned_text = soup.get_text()\n",
    "\n",
    "                        # Format date\n",
    "                        formatted_date = datetime.strptime(post[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                        writer.writerow({\n",
    "                            \"Topic ID\": post[\"topic_id\"],\n",
    "                            \"Username\": post[\"username\"],\n",
    "                            \"Post Created At\": formatted_date,\n",
    "                            \"Post Description\": cleaned_text,\n",
    "                            \"Post Number\": post[\"post_number\"]\n",
    "                        })\n",
    "\n",
    "                print(f\"Data from {url} successfully fetched and stored.\")\n",
    "                return posts[-1][\"post_number\"] + 1  # Increment post_number based on the last post number\n",
    "            else:\n",
    "                print(\"No new posts found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Error in response data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching data from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Specify API URL\n",
    "    api_url = \"https://forum.arbitrum.foundation/t/\"\n",
    "\n",
    "    # Read Topic IDs from the CSV file generated by ProposalsTopicSummary.py\n",
    "    with open(\"proposals_topics_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # Fetch and store data for each Topic ID\n",
    "            topic_id = row[\"Topic ID\"]\n",
    "            post_number = 0\n",
    "\n",
    "            while True:\n",
    "                last_post_number = fetch_and_store_data(f\"{api_url}{topic_id}/\", post_number)\n",
    "\n",
    "                if last_post_number is not None:\n",
    "                    post_number = last_post_number\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    print(\"All data successfully fetched and stored in proposals_post_data.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dao-Grant_Program User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# API URL for fetching user data\n",
    "user_api_url = \"https://forum.arbitrum.foundation/c/proposals/16.json\"\n",
    "# Specify CSV file path for user data\n",
    "user_csv_file_path = \"dgp_users_data.csv\"\n",
    "\n",
    "def fetch_users(api_url, page):\n",
    "    # Make request to fetch user data for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract user data from the response\n",
    "    users = data[\"users\"] if \"users\" in data else []\n",
    "    return users\n",
    "\n",
    "# Set to store processed user IDs\n",
    "processed_user_ids = set()\n",
    "\n",
    "# Write user data to CSV file\n",
    "with open(user_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as user_csv_file:\n",
    "    # Create CSV writer for user data\n",
    "    user_csv_writer = csv.writer(user_csv_file)\n",
    "\n",
    "    # Write user data header\n",
    "    user_header = [\"User Id\", \"Username\", \"Name\", \"Avtar Template\", \"Moderator\", \"Trust Level\"]\n",
    "\n",
    "    user_csv_writer.writerow(user_header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write user data until there are no more pages\n",
    "    while True:\n",
    "        # Fetch user data for the current page\n",
    "        users = fetch_users(user_api_url, page_num)\n",
    "\n",
    "        # Break the loop if no users are returned\n",
    "        if not users:\n",
    "            break\n",
    "\n",
    "        # Write user details, avoiding duplicates\n",
    "        for user_data in users:\n",
    "            user_id = user_data[\"id\"]\n",
    "\n",
    "            # Check if the user ID has already been processed\n",
    "            if user_id not in processed_user_ids:\n",
    "                # Add the user ID to the set of processed IDs\n",
    "                processed_user_ids.add(user_id)\n",
    "\n",
    "                # Initialize user data list with default values\n",
    "                user_row = [\n",
    "                    user_id,\n",
    "                    user_data[\"username\"],\n",
    "                    user_data[\"name\"],\n",
    "                    user_data[\"avatar_template\"],\n",
    "                    user_data.get(\"moderator\", False),\n",
    "                    user_data[\"trust_level\"],\n",
    "                ]\n",
    "\n",
    "                user_csv_writer.writerow(user_row)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"User data has been written to {user_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dao Grant Program Topic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://forum.arbitrum.foundation/c/proposals/16.json\"\n",
    "\n",
    "# Specify CSV file path\n",
    "csv_file_path = \"dgp_topics_data.csv\"\n",
    "\n",
    "def fetch_topics(api_url, page):\n",
    "    # Make request for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract topics from the current page\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    return topics\n",
    "\n",
    "# Function to format date and time\n",
    "def format_datetime(datetime_str):\n",
    "    if datetime_str:\n",
    "        # Convert the string to a datetime object\n",
    "        dt_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        # Format the datetime object as a string in the desired format\n",
    "        return dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    # Create CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write header\n",
    "    header = [\"Topic ID\", \"Title\", \"Fancy Title\", \"Slug\", \"Posts Count\", \"Reply Count\", \"Highest Post Number\",\n",
    "              \"Image URL\", \"Created At\", \"Last Posted At\", \"Views\", \"Like Count\", \"Pinned\", \"Unpinned\", \"Closed\", \"Visible\",\n",
    "              \"Tags\", \"Last Poster Username\", \"Category ID\", \"posters\", \"Original Poster ID\"]\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write topics until there are no more topics\n",
    "    while True:\n",
    "        topics = fetch_topics(api_url, page_num)\n",
    "\n",
    "        # Break the loop if no topics are returned\n",
    "        if not topics:\n",
    "            break\n",
    "\n",
    "        # Write topic details\n",
    "        for topic in topics:\n",
    "            # Extract the user IDs of the posters, excluding the original poster\n",
    "            user_ids_posters = [poster[\"user_id\"] for poster in topic[\"posters\"] if poster[\"user_id\"] != topic[\"posters\"][0][\"user_id\"]]\n",
    "\n",
    "            # Initialize data list with default values\n",
    "            data = [\n",
    "                topic[\"id\"],\n",
    "                topic[\"title\"],\n",
    "                topic[\"fancy_title\"],\n",
    "                topic[\"slug\"],\n",
    "                topic[\"posts_count\"],\n",
    "                topic[\"reply_count\"],\n",
    "                topic[\"highest_post_number\"],\n",
    "                topic[\"image_url\"],\n",
    "                format_datetime(topic[\"created_at\"]),  # Format Created At\n",
    "                format_datetime(topic[\"last_posted_at\"]),  # Format Last Posted At\n",
    "                topic[\"views\"],\n",
    "                topic[\"like_count\"],\n",
    "                topic[\"pinned\"],\n",
    "                topic[\"unpinned\"],\n",
    "                topic[\"closed\"],\n",
    "                topic[\"visible\"],\n",
    "                topic[\"tags\"],\n",
    "                topic[\"last_poster_username\"],\n",
    "                topic[\"category_id\"],\n",
    "                topic[\"posters\"],\n",
    "                topic[\"posters\"][0][\"user_id\"] if topic[\"posters\"] else None,\n",
    "            ]\n",
    "\n",
    "            csv_writer.writerow(data)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dao Grant Program Post Data (Pots available on each topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch and store data from a specific API endpoint\n",
    "def fetch_and_store_data(api_url, post_number):\n",
    "    url = f\"{api_url}{post_number}.json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"post_stream\" in data and \"posts\" in data[\"post_stream\"]:\n",
    "            posts = data[\"post_stream\"][\"posts\"]\n",
    "\n",
    "            # Remove duplicates based on post_number\n",
    "            posts = [post for post in posts if post[\"post_number\"] > post_number]\n",
    "\n",
    "            if posts:\n",
    "                # Write data to CSV file\n",
    "                with open(\"dgp_posts_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    fieldnames = [\"Topic ID\", \"Username\", \"Post Created At\", \"Post Description\", \"Post Number\"]\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    # Write headers if the file is empty\n",
    "                    if csv_file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    # Write posts data\n",
    "                    for post in posts:\n",
    "                        # Use BeautifulSoup to extract text from HTML\n",
    "                        soup = BeautifulSoup(post[\"cooked\"], \"html.parser\")\n",
    "                        cleaned_text = soup.get_text()\n",
    "\n",
    "                        # Format date\n",
    "                        formatted_date = datetime.strptime(post[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                        writer.writerow({\n",
    "                            \"Topic ID\": post[\"topic_id\"],\n",
    "                            \"Username\": post[\"username\"],\n",
    "                            \"Post Created At\": formatted_date,\n",
    "                            \"Post Description\": cleaned_text,\n",
    "                            \"Post Number\": post[\"post_number\"]\n",
    "                        })\n",
    "\n",
    "                print(f\"Data from {url} successfully fetched and stored.\")\n",
    "                return posts[-1][\"post_number\"] + 1  # Increment post_number based on the last post number\n",
    "            else:\n",
    "                print(\"No new posts found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Error in response data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching data from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Specify API URL\n",
    "    api_url = \"https://forum.arbitrum.foundation/t/\"\n",
    "\n",
    "    # Read Topic IDs from the CSV file generated by ProposalsTopicSummary.py\n",
    "    with open(\"dgp_topics_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # Fetch and store data for each Topic ID\n",
    "            topic_id = row[\"Topic ID\"]\n",
    "            post_number = 0\n",
    "\n",
    "            while True:\n",
    "                last_post_number = fetch_and_store_data(f\"{api_url}{topic_id}/\", post_number)\n",
    "\n",
    "                if last_post_number is not None:\n",
    "                    post_number = last_post_number\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    print(\"All data successfully fetched and stored in proposals_post_data.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grant Discussion User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# API URL for fetching user data\n",
    "user_api_url = \"https://forum.arbitrum.foundation/c/proposals/11.json\"\n",
    "# Specify CSV file path for user data\n",
    "user_csv_file_path = \"gd_users_data.csv\"\n",
    "\n",
    "def fetch_users(api_url, page):\n",
    "    # Make request to fetch user data for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract user data from the response\n",
    "    users = data[\"users\"] if \"users\" in data else []\n",
    "    return users\n",
    "\n",
    "# Set to store processed user IDs\n",
    "processed_user_ids = set()\n",
    "\n",
    "# Write user data to CSV file\n",
    "with open(user_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as user_csv_file:\n",
    "    # Create CSV writer for user data\n",
    "    user_csv_writer = csv.writer(user_csv_file)\n",
    "\n",
    "    # Write user data header\n",
    "    user_header = [\"User Id\", \"Username\", \"Name\", \"Avtar Template\", \"Moderator\", \"Trust Level\"]\n",
    "\n",
    "    user_csv_writer.writerow(user_header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write user data until there are no more pages\n",
    "    while True:\n",
    "        # Fetch user data for the current page\n",
    "        users = fetch_users(user_api_url, page_num)\n",
    "\n",
    "        # Break the loop if no users are returned\n",
    "        if not users:\n",
    "            break\n",
    "\n",
    "        # Write user details, avoiding duplicates\n",
    "        for user_data in users:\n",
    "            user_id = user_data[\"id\"]\n",
    "\n",
    "            # Check if the user ID has already been processed\n",
    "            if user_id not in processed_user_ids:\n",
    "                # Add the user ID to the set of processed IDs\n",
    "                processed_user_ids.add(user_id)\n",
    "\n",
    "                # Initialize user data list with default values\n",
    "                user_row = [\n",
    "                    user_id,\n",
    "                    user_data[\"username\"],\n",
    "                    user_data[\"name\"],\n",
    "                    user_data[\"avatar_template\"],\n",
    "                    user_data.get(\"moderator\", False),\n",
    "                    user_data[\"trust_level\"],\n",
    "                ]\n",
    "\n",
    "                user_csv_writer.writerow(user_row)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"User data has been written to {user_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grant Discussion Topics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://forum.arbitrum.foundation/c/proposals/11.json\"\n",
    "\n",
    "# Specify CSV file path\n",
    "csv_file_path = \"gd_topics_data.csv\"\n",
    "\n",
    "def fetch_topics(api_url, page):\n",
    "    # Make request for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract topics from the current page\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    return topics\n",
    "\n",
    "# Function to format date and time\n",
    "def format_datetime(datetime_str):\n",
    "    # Convert the string to a datetime object\n",
    "    dt_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    # Format the datetime object as a string in the desired format\n",
    "    return dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    # Create CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write header\n",
    "    header = [\"Topic ID\", \"Title\", \"Fancy Title\", \"Slug\", \"Posts Count\", \"Reply Count\", \"Highest Post Number\",\n",
    "              \"Image URL\", \"Created At\", \"Last Posted At\", \"Views\", \"Like Count\", \"Pinned\", \"Unpinned\", \"Closed\", \"Visible\",\n",
    "              \"Tags\", \"Last Poster Username\", \"Category ID\", \"posters\", \"Original Poster ID\"]\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write topics until there are no more topics\n",
    "    while True:\n",
    "        topics = fetch_topics(api_url, page_num)\n",
    "\n",
    "        # Break the loop if no topics are returned\n",
    "        if not topics:\n",
    "            break\n",
    "\n",
    "        # Write topic details\n",
    "        for topic in topics:\n",
    "            # Extract the user IDs of the posters, excluding the original poster\n",
    "            user_ids_posters = [poster[\"user_id\"] for poster in topic[\"posters\"] if poster[\"user_id\"] != topic[\"posters\"][0][\"user_id\"]]\n",
    "\n",
    "            # Initialize data list with default values\n",
    "            data = [\n",
    "                topic[\"id\"],\n",
    "                topic[\"title\"],\n",
    "                topic[\"fancy_title\"],\n",
    "                topic[\"slug\"],\n",
    "                topic[\"posts_count\"],\n",
    "                topic[\"reply_count\"],\n",
    "                topic[\"highest_post_number\"],\n",
    "                topic[\"image_url\"],\n",
    "                format_datetime(topic[\"created_at\"]),  # Format Created At\n",
    "                format_datetime(topic[\"last_posted_at\"]),  # Format Last Posted At\n",
    "                topic[\"views\"],\n",
    "                topic[\"like_count\"],\n",
    "                topic[\"pinned\"],\n",
    "                topic[\"unpinned\"],\n",
    "                topic[\"closed\"],\n",
    "                topic[\"visible\"],\n",
    "                topic[\"tags\"],\n",
    "                topic[\"last_poster_username\"],\n",
    "                topic[\"category_id\"],\n",
    "                topic[\"posters\"],\n",
    "                topic[\"posters\"][0][\"user_id\"] if topic[\"posters\"] else None,\n",
    "            ]\n",
    "\n",
    "            csv_writer.writerow(data)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grant Discussion Posts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch and store data from a specific API endpoint\n",
    "def fetch_and_store_data(api_url, post_number):\n",
    "    url = f\"{api_url}{post_number}.json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"post_stream\" in data and \"posts\" in data[\"post_stream\"]:\n",
    "            posts = data[\"post_stream\"][\"posts\"]\n",
    "\n",
    "            # Remove duplicates based on post_number\n",
    "            posts = [post for post in posts if post[\"post_number\"] > post_number]\n",
    "\n",
    "            if posts:\n",
    "                # Write data to CSV file\n",
    "                with open(\"gd_posts_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    fieldnames = [\"Topic ID\", \"Username\", \"Post Created At\", \"Post Description\", \"Post Number\"]\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    # Write headers if the file is empty\n",
    "                    if csv_file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    # Write posts data\n",
    "                    for post in posts:\n",
    "                        # Use BeautifulSoup to extract text from HTML\n",
    "                        soup = BeautifulSoup(post[\"cooked\"], \"html.parser\")\n",
    "                        cleaned_text = soup.get_text()\n",
    "\n",
    "                        # Format date\n",
    "                        formatted_date = datetime.strptime(post[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                        writer.writerow({\n",
    "                            \"Topic ID\": post[\"topic_id\"],\n",
    "                            \"Username\": post[\"username\"],\n",
    "                            \"Post Created At\": formatted_date,\n",
    "                            \"Post Description\": cleaned_text,\n",
    "                            \"Post Number\": post[\"post_number\"]\n",
    "                        })\n",
    "\n",
    "                print(f\"Data from {url} successfully fetched and stored.\")\n",
    "                return posts[-1][\"post_number\"] + 1  # Increment post_number based on the last post number\n",
    "            else:\n",
    "                print(\"No new posts found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Error in response data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching data from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Specify API URL\n",
    "    api_url = \"https://forum.arbitrum.foundation/t/\"\n",
    "\n",
    "    # Read Topic IDs from the CSV file generated by ProposalsTopicSummary.py\n",
    "    with open(\"gd_topics_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # Fetch and store data for each Topic ID\n",
    "            topic_id = row[\"Topic ID\"]\n",
    "            post_number = 0\n",
    "\n",
    "            while True:\n",
    "                last_post_number = fetch_and_store_data(f\"{api_url}{topic_id}/\", post_number)\n",
    "\n",
    "                if last_post_number is not None:\n",
    "                    post_number = last_post_number\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    print(\"All data successfully fetched and stored in proposals_post_data.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Governance User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# API URL for fetching user data\n",
    "user_api_url = \"https://forum.arbitrum.foundation/c/proposals/6.json\"\n",
    "# Specify CSV file path for user data\n",
    "user_csv_file_path = \"governance_users_data.csv\"\n",
    "\n",
    "def fetch_users(api_url, page):\n",
    "    # Make request to fetch user data for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract user data from the response\n",
    "    users = data[\"users\"] if \"users\" in data else []\n",
    "    return users\n",
    "\n",
    "# Set to store processed user IDs\n",
    "processed_user_ids = set()\n",
    "\n",
    "# Write user data to CSV file\n",
    "with open(user_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as user_csv_file:\n",
    "    # Create CSV writer for user data\n",
    "    user_csv_writer = csv.writer(user_csv_file)\n",
    "\n",
    "    # Write user data header\n",
    "    user_header = [\"User Id\", \"Username\", \"Name\", \"Avtar Template\", \"Moderator\", \"Trust Level\"]\n",
    "\n",
    "    user_csv_writer.writerow(user_header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write user data until there are no more pages\n",
    "    while True:\n",
    "        # Fetch user data for the current page\n",
    "        users = fetch_users(user_api_url, page_num)\n",
    "\n",
    "        # Break the loop if no users are returned\n",
    "        if not users:\n",
    "            break\n",
    "\n",
    "        # Write user details, avoiding duplicates\n",
    "        for user_data in users:\n",
    "            user_id = user_data[\"id\"]\n",
    "\n",
    "            # Check if the user ID has already been processed\n",
    "            if user_id not in processed_user_ids:\n",
    "                # Add the user ID to the set of processed IDs\n",
    "                processed_user_ids.add(user_id)\n",
    "\n",
    "                # Initialize user data list with default values\n",
    "                user_row = [\n",
    "                    user_id,\n",
    "                    user_data[\"username\"],\n",
    "                    user_data[\"name\"],\n",
    "                    user_data[\"avatar_template\"],\n",
    "                    user_data.get(\"moderator\", False),\n",
    "                    user_data[\"trust_level\"],\n",
    "                ]\n",
    "\n",
    "                user_csv_writer.writerow(user_row)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"User data has been written to {user_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Governance Topics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://forum.arbitrum.foundation/c/proposals/6.json\"\n",
    "\n",
    "# Specify CSV file path\n",
    "csv_file_path = \"governance_topics_data.csv\"\n",
    "\n",
    "def fetch_topics(api_url, page):\n",
    "    # Make request for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract topics from the current page\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    return topics\n",
    "\n",
    "# Function to format date and time\n",
    "def format_datetime(datetime_str):\n",
    "    # Convert the string to a datetime object\n",
    "    dt_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    # Format the datetime object as a string in the desired format\n",
    "    return dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    # Create CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write header\n",
    "    header = [\"Topic ID\", \"Title\", \"Fancy Title\", \"Slug\", \"Posts Count\", \"Reply Count\", \"Highest Post Number\",\n",
    "              \"Image URL\", \"Created At\", \"Last Posted At\", \"Views\", \"Like Count\", \"Pinned\", \"Unpinned\", \"Closed\", \"Visible\",\n",
    "              \"Tags\", \"Last Poster Username\", \"Category ID\", \"posters\", \"Original Poster ID\"]\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write topics until there are no more topics\n",
    "    while True:\n",
    "        topics = fetch_topics(api_url, page_num)\n",
    "\n",
    "        # Break the loop if no topics are returned\n",
    "        if not topics:\n",
    "            break\n",
    "\n",
    "        # Write topic details\n",
    "        for topic in topics:\n",
    "            # Extract the user IDs of the posters, excluding the original poster\n",
    "            user_ids_posters = [poster[\"user_id\"] for poster in topic[\"posters\"] if poster[\"user_id\"] != topic[\"posters\"][0][\"user_id\"]]\n",
    "\n",
    "            # Initialize data list with default values\n",
    "            data = [\n",
    "                topic[\"id\"],\n",
    "                topic[\"title\"],\n",
    "                topic[\"fancy_title\"],\n",
    "                topic[\"slug\"],\n",
    "                topic[\"posts_count\"],\n",
    "                topic[\"reply_count\"],\n",
    "                topic[\"highest_post_number\"],\n",
    "                topic[\"image_url\"],\n",
    "                format_datetime(topic[\"created_at\"]),  # Format Created At\n",
    "                format_datetime(topic[\"last_posted_at\"]),  # Format Last Posted At\n",
    "                topic[\"views\"],\n",
    "                topic[\"like_count\"],\n",
    "                topic[\"pinned\"],\n",
    "                topic[\"unpinned\"],\n",
    "                topic[\"closed\"],\n",
    "                topic[\"visible\"],\n",
    "                topic[\"tags\"],\n",
    "                topic[\"last_poster_username\"],\n",
    "                topic[\"category_id\"],\n",
    "                topic[\"posters\"],\n",
    "                topic[\"posters\"][0][\"user_id\"] if topic[\"posters\"] else None,\n",
    "            ]\n",
    "\n",
    "            csv_writer.writerow(data)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Governance Posts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch and store data from a specific API endpoint\n",
    "def fetch_and_store_data(api_url, post_number):\n",
    "    url = f\"{api_url}{post_number}.json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"post_stream\" in data and \"posts\" in data[\"post_stream\"]:\n",
    "            posts = data[\"post_stream\"][\"posts\"]\n",
    "\n",
    "            # Remove duplicates based on post_number\n",
    "            posts = [post for post in posts if post[\"post_number\"] > post_number]\n",
    "\n",
    "            if posts:\n",
    "                # Write data to CSV file\n",
    "                with open(\"governance_posts_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    fieldnames = [\"Topic ID\", \"Username\", \"Post Created At\", \"Post Description\", \"Post Number\"]\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    # Write headers if the file is empty\n",
    "                    if csv_file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    # Write posts data\n",
    "                    for post in posts:\n",
    "                        # Use BeautifulSoup to extract text from HTML\n",
    "                        soup = BeautifulSoup(post[\"cooked\"], \"html.parser\")\n",
    "                        cleaned_text = soup.get_text()\n",
    "\n",
    "                        # Format date\n",
    "                        formatted_date = datetime.strptime(post[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                        writer.writerow({\n",
    "                            \"Topic ID\": post[\"topic_id\"],\n",
    "                            \"Username\": post[\"username\"],\n",
    "                            \"Post Created At\": formatted_date,\n",
    "                            \"Post Description\": cleaned_text,\n",
    "                            \"Post Number\": post[\"post_number\"]\n",
    "                        })\n",
    "\n",
    "                print(f\"Data from {url} successfully fetched and stored.\")\n",
    "                return posts[-1][\"post_number\"] + 1  # Increment post_number based on the last post number\n",
    "            else:\n",
    "                print(\"No new posts found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Error in response data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching data from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Specify API URL\n",
    "    api_url = \"https://forum.arbitrum.foundation/t/\"\n",
    "\n",
    "    # Read Topic IDs from the CSV file generated by ProposalsTopicSummary.py\n",
    "    with open(\"governance_topics_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # Fetch and store data for each Topic ID\n",
    "            topic_id = row[\"Topic ID\"]\n",
    "            post_number = 0\n",
    "\n",
    "            while True:\n",
    "                last_post_number = fetch_and_store_data(f\"{api_url}{topic_id}/\", post_number)\n",
    "\n",
    "                if last_post_number is not None:\n",
    "                    post_number = last_post_number\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    print(\"All data successfully fetched and stored in proposals_post_data.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Rules User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# API URL for fetching user data\n",
    "user_api_url = \"https://forum.arbitrum.foundation/c/proposals/10.json\"\n",
    "# Specify CSV file path for user data\n",
    "user_csv_file_path = \"gr_users_data.csv\"\n",
    "\n",
    "def fetch_users(api_url, page):\n",
    "    # Make request to fetch user data for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract user data from the response\n",
    "    users = data[\"users\"] if \"users\" in data else []\n",
    "    return users\n",
    "\n",
    "# Set to store processed user IDs\n",
    "processed_user_ids = set()\n",
    "\n",
    "# Write user data to CSV file\n",
    "with open(user_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as user_csv_file:\n",
    "    # Create CSV writer for user data\n",
    "    user_csv_writer = csv.writer(user_csv_file)\n",
    "\n",
    "    # Write user data header\n",
    "    user_header = [\"User Id\", \"Username\", \"Name\", \"Avtar Template\", \"Moderator\", \"Trust Level\"]\n",
    "\n",
    "    user_csv_writer.writerow(user_header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write user data until there are no more pages\n",
    "    while True:\n",
    "        # Fetch user data for the current page\n",
    "        users = fetch_users(user_api_url, page_num)\n",
    "\n",
    "        # Break the loop if no users are returned\n",
    "        if not users:\n",
    "            break\n",
    "\n",
    "        # Write user details, avoiding duplicates\n",
    "        for user_data in users:\n",
    "            user_id = user_data[\"id\"]\n",
    "\n",
    "            # Check if the user ID has already been processed\n",
    "            if user_id not in processed_user_ids:\n",
    "                # Add the user ID to the set of processed IDs\n",
    "                processed_user_ids.add(user_id)\n",
    "\n",
    "                # Initialize user data list with default values\n",
    "                user_row = [\n",
    "                    user_id,\n",
    "                    user_data[\"username\"],\n",
    "                    user_data[\"name\"],\n",
    "                    user_data[\"avatar_template\"],\n",
    "                    user_data.get(\"moderator\", False),\n",
    "                    user_data[\"trust_level\"],\n",
    "                ]\n",
    "\n",
    "                user_csv_writer.writerow(user_row)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"User data has been written to {user_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Rules Topics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://forum.arbitrum.foundation/c/proposals/10.json\"\n",
    "\n",
    "# Specify CSV file path\n",
    "csv_file_path = \"gr_topics_data.csv\"\n",
    "\n",
    "def fetch_topics(api_url, page):\n",
    "    # Make request for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract topics from the current page\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    return topics\n",
    "\n",
    "# Function to format date and time\n",
    "def format_datetime(datetime_str):\n",
    "    # Convert the string to a datetime object\n",
    "    dt_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    # Format the datetime object as a string in the desired format\n",
    "    return dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    # Create CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write header\n",
    "    header = [\"Topic ID\", \"Title\", \"Fancy Title\", \"Slug\", \"Posts Count\", \"Reply Count\", \"Highest Post Number\",\n",
    "              \"Image URL\", \"Created At\", \"Last Posted At\", \"Views\", \"Like Count\", \"Pinned\", \"Unpinned\", \"Closed\", \"Visible\",\n",
    "              \"Tags\", \"Last Poster Username\", \"Category ID\", \"posters\", \"Original Poster ID\"]\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write topics until there are no more topics\n",
    "    while True:\n",
    "        topics = fetch_topics(api_url, page_num)\n",
    "\n",
    "        # Break the loop if no topics are returned\n",
    "        if not topics:\n",
    "            break\n",
    "\n",
    "        # Write topic details\n",
    "        for topic in topics:\n",
    "            # Extract the user IDs of the posters, excluding the original poster\n",
    "            user_ids_posters = [poster[\"user_id\"] for poster in topic[\"posters\"] if poster[\"user_id\"] != topic[\"posters\"][0][\"user_id\"]]\n",
    "\n",
    "            # Initialize data list with default values\n",
    "            data = [\n",
    "                topic[\"id\"],\n",
    "                topic[\"title\"],\n",
    "                topic[\"fancy_title\"],\n",
    "                topic[\"slug\"],\n",
    "                topic[\"posts_count\"],\n",
    "                topic[\"reply_count\"],\n",
    "                topic[\"highest_post_number\"],\n",
    "                topic[\"image_url\"],\n",
    "                format_datetime(topic[\"created_at\"]),  # Format Created At\n",
    "                format_datetime(topic[\"last_posted_at\"]),  # Format Last Posted At\n",
    "                topic[\"views\"],\n",
    "                topic[\"like_count\"],\n",
    "                topic[\"pinned\"],\n",
    "                topic[\"unpinned\"],\n",
    "                topic[\"closed\"],\n",
    "                topic[\"visible\"],\n",
    "                topic[\"tags\"],\n",
    "                topic[\"last_poster_username\"],\n",
    "                topic[\"category_id\"],\n",
    "                topic[\"posters\"],\n",
    "                topic[\"posters\"][0][\"user_id\"] if topic[\"posters\"] else None,\n",
    "            ]\n",
    "\n",
    "            csv_writer.writerow(data)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Rules Posts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch and store data from a specific API endpoint\n",
    "def fetch_and_store_data(api_url, post_number):\n",
    "    url = f\"{api_url}{post_number}.json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"post_stream\" in data and \"posts\" in data[\"post_stream\"]:\n",
    "            posts = data[\"post_stream\"][\"posts\"]\n",
    "\n",
    "            # Remove duplicates based on post_number\n",
    "            posts = [post for post in posts if post[\"post_number\"] > post_number]\n",
    "\n",
    "            if posts:\n",
    "                # Write data to CSV file\n",
    "                with open(\"gr_posts_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    fieldnames = [\"Topic ID\", \"Username\", \"Post Created At\", \"Post Description\", \"Post Number\"]\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    # Write headers if the file is empty\n",
    "                    if csv_file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    # Write posts data\n",
    "                    for post in posts:\n",
    "                        # Use BeautifulSoup to extract text from HTML\n",
    "                        soup = BeautifulSoup(post[\"cooked\"], \"html.parser\")\n",
    "                        cleaned_text = soup.get_text()\n",
    "\n",
    "                        # Format date\n",
    "                        formatted_date = datetime.strptime(post[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                        writer.writerow({\n",
    "                            \"Topic ID\": post[\"topic_id\"],\n",
    "                            \"Username\": post[\"username\"],\n",
    "                            \"Post Created At\": formatted_date,\n",
    "                            \"Post Description\": cleaned_text,\n",
    "                            \"Post Number\": post[\"post_number\"]\n",
    "                        })\n",
    "\n",
    "                print(f\"Data from {url} successfully fetched and stored.\")\n",
    "                return posts[-1][\"post_number\"] + 1  # Increment post_number based on the last post number\n",
    "            else:\n",
    "                print(\"No new posts found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Error in response data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching data from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Specify API URL\n",
    "    api_url = \"https://forum.arbitrum.foundation/t/\"\n",
    "\n",
    "    # Read Topic IDs from the CSV file generated by ProposalsTopicSummary.py\n",
    "    with open(\"gr_topics_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # Fetch and store data for each Topic ID\n",
    "            topic_id = row[\"Topic ID\"]\n",
    "            post_number = 0\n",
    "\n",
    "            while True:\n",
    "                last_post_number = fetch_and_store_data(f\"{api_url}{topic_id}/\", post_number)\n",
    "\n",
    "                if last_post_number is not None:\n",
    "                    post_number = last_post_number\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    print(\"All data successfully fetched and stored in proposals_post_data.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security Council Electoion User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# API URL for fetching user data\n",
    "user_api_url = \"https://forum.arbitrum.foundation/c/proposals/12.json\"\n",
    "# Specify CSV file path for user data\n",
    "user_csv_file_path = \"sce_users_data.csv\"\n",
    "\n",
    "def fetch_users(api_url, page):\n",
    "    # Make request to fetch user data for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract user data from the response\n",
    "    users = data[\"users\"] if \"users\" in data else []\n",
    "    return users\n",
    "\n",
    "# Set to store processed user IDs\n",
    "processed_user_ids = set()\n",
    "\n",
    "# Write user data to CSV file\n",
    "with open(user_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as user_csv_file:\n",
    "    # Create CSV writer for user data\n",
    "    user_csv_writer = csv.writer(user_csv_file)\n",
    "\n",
    "    # Write user data header\n",
    "    user_header = [\"User Id\", \"Username\", \"Name\", \"Avtar Template\", \"Moderator\", \"Trust Level\"]\n",
    "\n",
    "    user_csv_writer.writerow(user_header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write user data until there are no more pages\n",
    "    while True:\n",
    "        # Fetch user data for the current page\n",
    "        users = fetch_users(user_api_url, page_num)\n",
    "\n",
    "        # Break the loop if no users are returned\n",
    "        if not users:\n",
    "            break\n",
    "\n",
    "        # Write user details, avoiding duplicates\n",
    "        for user_data in users:\n",
    "            user_id = user_data[\"id\"]\n",
    "\n",
    "            # Check if the user ID has already been processed\n",
    "            if user_id not in processed_user_ids:\n",
    "                # Add the user ID to the set of processed IDs\n",
    "                processed_user_ids.add(user_id)\n",
    "\n",
    "                # Initialize user data list with default values\n",
    "                user_row = [\n",
    "                    user_id,\n",
    "                    user_data[\"username\"],\n",
    "                    user_data[\"name\"],\n",
    "                    user_data[\"avatar_template\"],\n",
    "                    user_data.get(\"moderator\", False),\n",
    "                    user_data[\"trust_level\"],\n",
    "                ]\n",
    "\n",
    "                user_csv_writer.writerow(user_row)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"User data has been written to {user_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security Council Electoion Topics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://forum.arbitrum.foundation/c/proposals/12.json\"\n",
    "\n",
    "# Specify CSV file path\n",
    "csv_file_path = \"sce_topics_data.csv\"\n",
    "\n",
    "def fetch_topics(api_url, page):\n",
    "    # Make request for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract topics from the current page\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    return topics\n",
    "\n",
    "# Function to format date and time\n",
    "def format_datetime(datetime_str):\n",
    "    # Convert the string to a datetime object\n",
    "    dt_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    # Format the datetime object as a string in the desired format\n",
    "    return dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    # Create CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write header\n",
    "    header = [\"Topic ID\", \"Title\", \"Fancy Title\", \"Slug\", \"Posts Count\", \"Reply Count\", \"Highest Post Number\",\n",
    "              \"Image URL\", \"Created At\", \"Last Posted At\", \"Views\", \"Like Count\", \"Pinned\", \"Unpinned\", \"Closed\", \"Visible\",\n",
    "              \"Tags\", \"Last Poster Username\", \"Category ID\", \"posters\", \"Original Poster ID\"]\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write topics until there are no more topics\n",
    "    while True:\n",
    "        topics = fetch_topics(api_url, page_num)\n",
    "\n",
    "        # Break the loop if no topics are returned\n",
    "        if not topics:\n",
    "            break\n",
    "\n",
    "        # Write topic details\n",
    "        for topic in topics:\n",
    "            # Extract the user IDs of the posters, excluding the original poster\n",
    "            user_ids_posters = [poster[\"user_id\"] for poster in topic[\"posters\"] if poster[\"user_id\"] != topic[\"posters\"][0][\"user_id\"]]\n",
    "\n",
    "            # Initialize data list with default values\n",
    "            data = [\n",
    "                topic[\"id\"],\n",
    "                topic[\"title\"],\n",
    "                topic[\"fancy_title\"],\n",
    "                topic[\"slug\"],\n",
    "                topic[\"posts_count\"],\n",
    "                topic[\"reply_count\"],\n",
    "                topic[\"highest_post_number\"],\n",
    "                topic[\"image_url\"],\n",
    "                format_datetime(topic[\"created_at\"]),  # Format Created At\n",
    "                format_datetime(topic[\"last_posted_at\"]),  # Format Last Posted At\n",
    "                topic[\"views\"],\n",
    "                topic[\"like_count\"],\n",
    "                topic[\"pinned\"],\n",
    "                topic[\"unpinned\"],\n",
    "                topic[\"closed\"],\n",
    "                topic[\"visible\"],\n",
    "                topic[\"tags\"],\n",
    "                topic[\"last_poster_username\"],\n",
    "                topic[\"category_id\"],\n",
    "                topic[\"posters\"],\n",
    "                topic[\"posters\"][0][\"user_id\"] if topic[\"posters\"] else None,\n",
    "            ]\n",
    "\n",
    "            csv_writer.writerow(data)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security Council Electoion Posts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch and store data from a specific API endpoint\n",
    "def fetch_and_store_data(api_url, post_number):\n",
    "    url = f\"{api_url}{post_number}.json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"post_stream\" in data and \"posts\" in data[\"post_stream\"]:\n",
    "            posts = data[\"post_stream\"][\"posts\"]\n",
    "\n",
    "            # Remove duplicates based on post_number\n",
    "            posts = [post for post in posts if post[\"post_number\"] > post_number]\n",
    "\n",
    "            if posts:\n",
    "                # Write data to CSV file\n",
    "                with open(\"sce_posts_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    fieldnames = [\"Topic ID\", \"Username\", \"Post Created At\", \"Post Description\", \"Post Number\"]\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    # Write headers if the file is empty\n",
    "                    if csv_file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    # Write posts data\n",
    "                    for post in posts:\n",
    "                        # Use BeautifulSoup to extract text from HTML\n",
    "                        soup = BeautifulSoup(post[\"cooked\"], \"html.parser\")\n",
    "                        cleaned_text = soup.get_text()\n",
    "\n",
    "                        # Format date\n",
    "                        formatted_date = datetime.strptime(post[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                        writer.writerow({\n",
    "                            \"Topic ID\": post[\"topic_id\"],\n",
    "                            \"Username\": post[\"username\"],\n",
    "                            \"Post Created At\": formatted_date,\n",
    "                            \"Post Description\": cleaned_text,\n",
    "                            \"Post Number\": post[\"post_number\"]\n",
    "                        })\n",
    "\n",
    "                print(f\"Data from {url} successfully fetched and stored.\")\n",
    "                return posts[-1][\"post_number\"] + 1  # Increment post_number based on the last post number\n",
    "            else:\n",
    "                print(\"No new posts found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Error in response data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching data from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Specify API URL\n",
    "    api_url = \"https://forum.arbitrum.foundation/t/\"\n",
    "\n",
    "    # Read Topic IDs from the CSV file generated by ProposalsTopicSummary.py\n",
    "    with open(\"sce_topics_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # Fetch and store data for each Topic ID\n",
    "            topic_id = row[\"Topic ID\"]\n",
    "            post_number = 0\n",
    "\n",
    "            while True:\n",
    "                last_post_number = fetch_and_store_data(f\"{api_url}{topic_id}/\", post_number)\n",
    "\n",
    "                if last_post_number is not None:\n",
    "                    post_number = last_post_number\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    print(\"All data successfully fetched and stored in proposals_post_data.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum Dao Chains User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# API URL for fetching user data\n",
    "user_api_url = \"https://forum.arbitrum.foundation/c/proposals/5.json\"\n",
    "# Specify CSV file path for user data\n",
    "user_csv_file_path = \"adc_users_data.csv\"\n",
    "\n",
    "def fetch_users(api_url, page):\n",
    "    # Make request to fetch user data for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract user data from the response\n",
    "    users = data[\"users\"] if \"users\" in data else []\n",
    "    return users\n",
    "\n",
    "# Set to store processed user IDs\n",
    "processed_user_ids = set()\n",
    "\n",
    "# Write user data to CSV file\n",
    "with open(user_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as user_csv_file:\n",
    "    # Create CSV writer for user data\n",
    "    user_csv_writer = csv.writer(user_csv_file)\n",
    "\n",
    "    # Write user data header\n",
    "    user_header = [\"User Id\", \"Username\", \"Name\", \"Avtar Template\", \"Moderator\", \"Trust Level\"]\n",
    "\n",
    "    user_csv_writer.writerow(user_header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write user data until there are no more pages\n",
    "    while True:\n",
    "        # Fetch user data for the current page\n",
    "        users = fetch_users(user_api_url, page_num)\n",
    "\n",
    "        # Break the loop if no users are returned\n",
    "        if not users:\n",
    "            break\n",
    "\n",
    "        # Write user details, avoiding duplicates\n",
    "        for user_data in users:\n",
    "            user_id = user_data[\"id\"]\n",
    "\n",
    "            # Check if the user ID has already been processed\n",
    "            if user_id not in processed_user_ids:\n",
    "                # Add the user ID to the set of processed IDs\n",
    "                processed_user_ids.add(user_id)\n",
    "\n",
    "                # Initialize user data list with default values\n",
    "                user_row = [\n",
    "                    user_id,\n",
    "                    user_data[\"username\"],\n",
    "                    user_data[\"name\"],\n",
    "                    user_data[\"avatar_template\"],\n",
    "                    user_data.get(\"moderator\", False),\n",
    "                    user_data[\"trust_level\"],\n",
    "                ]\n",
    "\n",
    "                user_csv_writer.writerow(user_row)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"User data has been written to {user_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum Dao Chains Topics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://forum.arbitrum.foundation/c/proposals/5.json\"\n",
    "\n",
    "# Specify CSV file path\n",
    "csv_file_path = \"adc_topics_data.csv\"\n",
    "\n",
    "def fetch_topics(api_url, page):\n",
    "    # Make request for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract topics from the current page\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    return topics\n",
    "\n",
    "# Function to format date and time\n",
    "def format_datetime(datetime_str):\n",
    "    # Convert the string to a datetime object\n",
    "    dt_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    # Format the datetime object as a string in the desired format\n",
    "    return dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    # Create CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write header\n",
    "    header = [\"Topic ID\", \"Title\", \"Fancy Title\", \"Slug\", \"Posts Count\", \"Reply Count\", \"Highest Post Number\",\n",
    "              \"Image URL\", \"Created At\", \"Last Posted At\", \"Views\", \"Like Count\", \"Pinned\", \"Unpinned\", \"Closed\", \"Visible\",\n",
    "              \"Tags\", \"Last Poster Username\", \"Category ID\", \"posters\", \"Original Poster ID\"]\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write topics until there are no more topics\n",
    "    while True:\n",
    "        topics = fetch_topics(api_url, page_num)\n",
    "\n",
    "        # Break the loop if no topics are returned\n",
    "        if not topics:\n",
    "            break\n",
    "\n",
    "        # Write topic details\n",
    "        for topic in topics:\n",
    "            # Extract the user IDs of the posters, excluding the original poster\n",
    "            user_ids_posters = [poster[\"user_id\"] for poster in topic[\"posters\"] if poster[\"user_id\"] != topic[\"posters\"][0][\"user_id\"]]\n",
    "\n",
    "            # Initialize data list with default values\n",
    "            data = [\n",
    "                topic[\"id\"],\n",
    "                topic[\"title\"],\n",
    "                topic[\"fancy_title\"],\n",
    "                topic[\"slug\"],\n",
    "                topic[\"posts_count\"],\n",
    "                topic[\"reply_count\"],\n",
    "                topic[\"highest_post_number\"],\n",
    "                topic[\"image_url\"],\n",
    "                format_datetime(topic[\"created_at\"]),  # Format Created At\n",
    "                format_datetime(topic[\"last_posted_at\"]),  # Format Last Posted At\n",
    "                topic[\"views\"],\n",
    "                topic[\"like_count\"],\n",
    "                topic[\"pinned\"],\n",
    "                topic[\"unpinned\"],\n",
    "                topic[\"closed\"],\n",
    "                topic[\"visible\"],\n",
    "                topic[\"tags\"],\n",
    "                topic[\"last_poster_username\"],\n",
    "                topic[\"category_id\"],\n",
    "                topic[\"posters\"],\n",
    "                topic[\"posters\"][0][\"user_id\"] if topic[\"posters\"] else None,\n",
    "            ]\n",
    "\n",
    "            csv_writer.writerow(data)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum Dao Chains Posts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch and store data from a specific API endpoint\n",
    "def fetch_and_store_data(api_url, post_number):\n",
    "    url = f\"{api_url}{post_number}.json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"post_stream\" in data and \"posts\" in data[\"post_stream\"]:\n",
    "            posts = data[\"post_stream\"][\"posts\"]\n",
    "\n",
    "            # Remove duplicates based on post_number\n",
    "            posts = [post for post in posts if post[\"post_number\"] > post_number]\n",
    "\n",
    "            if posts:\n",
    "                # Write data to CSV file\n",
    "                with open(\"adc_posts_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    fieldnames = [\"Topic ID\", \"Username\", \"Post Created At\", \"Post Description\", \"Post Number\"]\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    # Write headers if the file is empty\n",
    "                    if csv_file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    # Write posts data\n",
    "                    for post in posts:\n",
    "                        # Use BeautifulSoup to extract text from HTML\n",
    "                        soup = BeautifulSoup(post[\"cooked\"], \"html.parser\")\n",
    "                        cleaned_text = soup.get_text()\n",
    "\n",
    "                        # Format date\n",
    "                        formatted_date = datetime.strptime(post[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                        writer.writerow({\n",
    "                            \"Topic ID\": post[\"topic_id\"],\n",
    "                            \"Username\": post[\"username\"],\n",
    "                            \"Post Created At\": formatted_date,\n",
    "                            \"Post Description\": cleaned_text,\n",
    "                            \"Post Number\": post[\"post_number\"]\n",
    "                        })\n",
    "\n",
    "                print(f\"Data from {url} successfully fetched and stored.\")\n",
    "                return posts[-1][\"post_number\"] + 1  # Increment post_number based on the last post number\n",
    "            else:\n",
    "                print(\"No new posts found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Error in response data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching data from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Specify API URL\n",
    "    api_url = \"https://forum.arbitrum.foundation/t/\"\n",
    "\n",
    "    # Read Topic IDs from the CSV file generated by ProposalsTopicSummary.py\n",
    "    with open(\"adc_topics_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # Fetch and store data for each Topic ID\n",
    "            topic_id = row[\"Topic ID\"]\n",
    "            post_number = 0\n",
    "\n",
    "            while True:\n",
    "                last_post_number = fetch_and_store_data(f\"{api_url}{topic_id}/\", post_number)\n",
    "\n",
    "                if last_post_number is not None:\n",
    "                    post_number = last_post_number\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    print(\"All data successfully fetched and stored in proposals_post_data.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum GovHack Submissions Users Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# API URL for fetching user data\n",
    "user_api_url = \"https://forum.arbitrum.foundation/c/proposals/26.json\"\n",
    "# Specify CSV file path for user data\n",
    "user_csv_file_path = \"aghs_users_data.csv\"\n",
    "\n",
    "def fetch_users(api_url, page):\n",
    "    # Make request to fetch user data for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract user data from the response\n",
    "    users = data[\"users\"] if \"users\" in data else []\n",
    "    return users\n",
    "\n",
    "# Set to store processed user IDs\n",
    "processed_user_ids = set()\n",
    "\n",
    "# Write user data to CSV file\n",
    "with open(user_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as user_csv_file:\n",
    "    # Create CSV writer for user data\n",
    "    user_csv_writer = csv.writer(user_csv_file)\n",
    "\n",
    "    # Write user data header\n",
    "    user_header = [\"User Id\", \"Username\", \"Name\", \"Avtar Template\", \"Moderator\", \"Trust Level\"]\n",
    "\n",
    "    user_csv_writer.writerow(user_header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write user data until there are no more pages\n",
    "    while True:\n",
    "        # Fetch user data for the current page\n",
    "        users = fetch_users(user_api_url, page_num)\n",
    "\n",
    "        # Break the loop if no users are returned\n",
    "        if not users:\n",
    "            break\n",
    "\n",
    "        # Write user details, avoiding duplicates\n",
    "        for user_data in users:\n",
    "            user_id = user_data[\"id\"]\n",
    "\n",
    "            # Check if the user ID has already been processed\n",
    "            if user_id not in processed_user_ids:\n",
    "                # Add the user ID to the set of processed IDs\n",
    "                processed_user_ids.add(user_id)\n",
    "\n",
    "                # Initialize user data list with default values\n",
    "                user_row = [\n",
    "                    user_id,\n",
    "                    user_data[\"username\"],\n",
    "                    user_data[\"name\"],\n",
    "                    user_data[\"avatar_template\"],\n",
    "                    user_data.get(\"moderator\", False),\n",
    "                    user_data[\"trust_level\"],\n",
    "                ]\n",
    "\n",
    "                user_csv_writer.writerow(user_row)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"User data has been written to {user_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum GovHack Submissions Topics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://forum.arbitrum.foundation/c/proposals/26.json\"\n",
    "\n",
    "# Specify CSV file path\n",
    "csv_file_path = \"aghs_topics_data.csv\"\n",
    "\n",
    "def fetch_topics(api_url, page):\n",
    "    # Make request for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract topics from the current page\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    return topics\n",
    "\n",
    "# Function to format date and time\n",
    "def format_datetime(datetime_str):\n",
    "    if datetime_str:\n",
    "        # Convert the string to a datetime object\n",
    "        dt_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        # Format the datetime object as a string in the desired format\n",
    "        return dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    # Create CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write header\n",
    "    header = [\"Topic ID\", \"Title\", \"Fancy Title\", \"Slug\", \"Posts Count\", \"Reply Count\", \"Highest Post Number\",\n",
    "              \"Image URL\", \"Created At\", \"Last Posted At\", \"Views\", \"Like Count\", \"Pinned\", \"Unpinned\", \"Closed\", \"Visible\",\n",
    "              \"Tags\", \"Last Poster Username\", \"Category ID\", \"posters\", \"Original Poster ID\"]\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write topics until there are no more topics\n",
    "    while True:\n",
    "        topics = fetch_topics(api_url, page_num)\n",
    "\n",
    "        # Break the loop if no topics are returned\n",
    "        if not topics:\n",
    "            break\n",
    "\n",
    "        # Write topic details\n",
    "        for topic in topics:\n",
    "            # Extract the user IDs of the posters, excluding the original poster\n",
    "            user_ids_posters = [poster[\"user_id\"] for poster in topic[\"posters\"] if poster[\"user_id\"] != topic[\"posters\"][0][\"user_id\"]]\n",
    "\n",
    "            # Initialize data list with default values\n",
    "            data = [\n",
    "                topic[\"id\"],\n",
    "                topic[\"title\"],\n",
    "                topic[\"fancy_title\"],\n",
    "                topic[\"slug\"],\n",
    "                topic[\"posts_count\"],\n",
    "                topic[\"reply_count\"],\n",
    "                topic[\"highest_post_number\"],\n",
    "                topic[\"image_url\"],\n",
    "                format_datetime(topic[\"created_at\"]),  # Format Created At\n",
    "                format_datetime(topic[\"last_posted_at\"]),  # Format Last Posted At\n",
    "                topic[\"views\"],\n",
    "                topic[\"like_count\"],\n",
    "                topic[\"pinned\"],\n",
    "                topic[\"unpinned\"],\n",
    "                topic[\"closed\"],\n",
    "                topic[\"visible\"],\n",
    "                topic[\"tags\"],\n",
    "                topic[\"last_poster_username\"],\n",
    "                topic[\"category_id\"],\n",
    "                topic[\"posters\"],\n",
    "                topic[\"posters\"][0][\"user_id\"] if topic[\"posters\"] else None,\n",
    "            ]\n",
    "\n",
    "            csv_writer.writerow(data)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum GovHack Submissions Posts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch and store data from a specific API endpoint\n",
    "def fetch_and_store_data(api_url, post_number):\n",
    "    url = f\"{api_url}{post_number}.json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"post_stream\" in data and \"posts\" in data[\"post_stream\"]:\n",
    "            posts = data[\"post_stream\"][\"posts\"]\n",
    "\n",
    "            # Remove duplicates based on post_number\n",
    "            posts = [post for post in posts if post[\"post_number\"] > post_number]\n",
    "\n",
    "            if posts:\n",
    "                # Write data to CSV file\n",
    "                with open(\"aghs_posts_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    fieldnames = [\"Topic ID\", \"Username\", \"Post Created At\", \"Post Description\", \"Post Number\"]\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    # Write headers if the file is empty\n",
    "                    if csv_file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    # Write posts data\n",
    "                    for post in posts:\n",
    "                        # Use BeautifulSoup to extract text from HTML\n",
    "                        soup = BeautifulSoup(post[\"cooked\"], \"html.parser\")\n",
    "                        cleaned_text = soup.get_text()\n",
    "\n",
    "                        # Format date\n",
    "                        formatted_date = datetime.strptime(post[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                        writer.writerow({\n",
    "                            \"Topic ID\": post[\"topic_id\"],\n",
    "                            \"Username\": post[\"username\"],\n",
    "                            \"Post Created At\": formatted_date,\n",
    "                            \"Post Description\": cleaned_text,\n",
    "                            \"Post Number\": post[\"post_number\"]\n",
    "                        })\n",
    "\n",
    "                print(f\"Data from {url} successfully fetched and stored.\")\n",
    "                return posts[-1][\"post_number\"] + 1  # Increment post_number based on the last post number\n",
    "            else:\n",
    "                print(\"No new posts found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Error in response data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching data from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Specify API URL\n",
    "    api_url = \"https://forum.arbitrum.foundation/t/\"\n",
    "\n",
    "    # Read Topic IDs from the CSV file generated by ProposalsTopicSummary.py\n",
    "    with open(\"aghs_topics_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # Fetch and store data for each Topic ID\n",
    "            topic_id = row[\"Topic ID\"]\n",
    "            post_number = 0\n",
    "\n",
    "            while True:\n",
    "                last_post_number = fetch_and_store_data(f\"{api_url}{topic_id}/\", post_number)\n",
    "\n",
    "                if last_post_number is not None:\n",
    "                    post_number = last_post_number\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    print(\"All data successfully fetched and stored in proposals_post_data.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procurement Committee Users Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# API URL for fetching user data\n",
    "user_api_url = \"https://forum.arbitrum.foundation/c/proposals/26.json\"\n",
    "# Specify CSV file path for user data\n",
    "user_csv_file_path = \"pc_users_data.csv\"\n",
    "\n",
    "def fetch_users(api_url, page):\n",
    "    # Make request to fetch user data for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract user data from the response\n",
    "    users = data[\"users\"] if \"users\" in data else []\n",
    "    return users\n",
    "\n",
    "# Set to store processed user IDs\n",
    "processed_user_ids = set()\n",
    "\n",
    "# Write user data to CSV file\n",
    "with open(user_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as user_csv_file:\n",
    "    # Create CSV writer for user data\n",
    "    user_csv_writer = csv.writer(user_csv_file)\n",
    "\n",
    "    # Write user data header\n",
    "    user_header = [\"User Id\", \"Username\", \"Name\", \"Avtar Template\", \"Moderator\", \"Trust Level\"]\n",
    "\n",
    "    user_csv_writer.writerow(user_header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write user data until there are no more pages\n",
    "    while True:\n",
    "        # Fetch user data for the current page\n",
    "        users = fetch_users(user_api_url, page_num)\n",
    "\n",
    "        # Break the loop if no users are returned\n",
    "        if not users:\n",
    "            break\n",
    "\n",
    "        # Write user details, avoiding duplicates\n",
    "        for user_data in users:\n",
    "            user_id = user_data[\"id\"]\n",
    "\n",
    "            # Check if the user ID has already been processed\n",
    "            if user_id not in processed_user_ids:\n",
    "                # Add the user ID to the set of processed IDs\n",
    "                processed_user_ids.add(user_id)\n",
    "\n",
    "                # Initialize user data list with default values\n",
    "                user_row = [\n",
    "                    user_id,\n",
    "                    user_data[\"username\"],\n",
    "                    user_data[\"name\"],\n",
    "                    user_data[\"avatar_template\"],\n",
    "                    user_data.get(\"moderator\", False),\n",
    "                    user_data[\"trust_level\"],\n",
    "                ]\n",
    "\n",
    "                user_csv_writer.writerow(user_row)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"User data has been written to {user_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procurement Committee Topics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://forum.arbitrum.foundation/c/proposals/27.json\"\n",
    "\n",
    "# Specify CSV file path\n",
    "csv_file_path = \"pc_topics_data.csv\"\n",
    "\n",
    "def fetch_topics(api_url, page):\n",
    "    # Make request for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract topics from the current page\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    return topics\n",
    "\n",
    "# Function to format date and time\n",
    "def format_datetime(datetime_str):\n",
    "    if datetime_str:\n",
    "        # Convert the string to a datetime object\n",
    "        dt_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        # Format the datetime object as a string in the desired format\n",
    "        return dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    # Create CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write header\n",
    "    header = [\"Topic ID\", \"Title\", \"Fancy Title\", \"Slug\", \"Posts Count\", \"Reply Count\", \"Highest Post Number\",\n",
    "              \"Image URL\", \"Created At\", \"Last Posted At\", \"Views\", \"Like Count\", \"Pinned\", \"Unpinned\", \"Closed\", \"Visible\",\n",
    "              \"Tags\", \"Last Poster Username\", \"Category ID\", \"posters\", \"Original Poster ID\"]\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write topics until there are no more topics\n",
    "    while True:\n",
    "        topics = fetch_topics(api_url, page_num)\n",
    "\n",
    "        # Break the loop if no topics are returned\n",
    "        if not topics:\n",
    "            break\n",
    "\n",
    "        # Write topic details\n",
    "        for topic in topics:\n",
    "            # Extract the user IDs of the posters, excluding the original poster\n",
    "            user_ids_posters = [poster[\"user_id\"] for poster in topic[\"posters\"] if poster[\"user_id\"] != topic[\"posters\"][0][\"user_id\"]]\n",
    "\n",
    "            # Initialize data list with default values\n",
    "            data = [\n",
    "                topic[\"id\"],\n",
    "                topic[\"title\"],\n",
    "                topic[\"fancy_title\"],\n",
    "                topic[\"slug\"],\n",
    "                topic[\"posts_count\"],\n",
    "                topic[\"reply_count\"],\n",
    "                topic[\"highest_post_number\"],\n",
    "                topic[\"image_url\"],\n",
    "                format_datetime(topic[\"created_at\"]),  # Format Created At\n",
    "                format_datetime(topic[\"last_posted_at\"]),  # Format Last Posted At\n",
    "                topic[\"views\"],\n",
    "                topic[\"like_count\"],\n",
    "                topic[\"pinned\"],\n",
    "                topic[\"unpinned\"],\n",
    "                topic[\"closed\"],\n",
    "                topic[\"visible\"],\n",
    "                topic[\"tags\"],\n",
    "                topic[\"last_poster_username\"],\n",
    "                topic[\"category_id\"],\n",
    "                topic[\"posters\"],\n",
    "                topic[\"posters\"][0][\"user_id\"] if topic[\"posters\"] else None,\n",
    "            ]\n",
    "\n",
    "            csv_writer.writerow(data)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procurement Committee Posts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch and store data from a specific API endpoint\n",
    "def fetch_and_store_data(api_url, post_number):\n",
    "    url = f\"{api_url}{post_number}.json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"post_stream\" in data and \"posts\" in data[\"post_stream\"]:\n",
    "            posts = data[\"post_stream\"][\"posts\"]\n",
    "\n",
    "            # Remove duplicates based on post_number\n",
    "            posts = [post for post in posts if post[\"post_number\"] > post_number]\n",
    "\n",
    "            if posts:\n",
    "                # Write data to CSV file\n",
    "                with open(\"pc_posts_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    fieldnames = [\"Topic ID\", \"Username\", \"Post Created At\", \"Post Description\", \"Post Number\"]\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    # Write headers if the file is empty\n",
    "                    if csv_file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    # Write posts data\n",
    "                    for post in posts:\n",
    "                        # Use BeautifulSoup to extract text from HTML\n",
    "                        soup = BeautifulSoup(post[\"cooked\"], \"html.parser\")\n",
    "                        cleaned_text = soup.get_text()\n",
    "\n",
    "                        # Format date\n",
    "                        formatted_date = datetime.strptime(post[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                        writer.writerow({\n",
    "                            \"Topic ID\": post[\"topic_id\"],\n",
    "                            \"Username\": post[\"username\"],\n",
    "                            \"Post Created At\": formatted_date,\n",
    "                            \"Post Description\": cleaned_text,\n",
    "                            \"Post Number\": post[\"post_number\"]\n",
    "                        })\n",
    "\n",
    "                print(f\"Data from {url} successfully fetched and stored.\")\n",
    "                return posts[-1][\"post_number\"] + 1  # Increment post_number based on the last post number\n",
    "            else:\n",
    "                print(\"No new posts found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Error in response data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching data from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Specify API URL\n",
    "    api_url = \"https://forum.arbitrum.foundation/t/\"\n",
    "\n",
    "    # Read Topic IDs from the CSV file generated by ProposalsTopicSummary.py\n",
    "    with open(\"pc_topics_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # Fetch and store data for each Topic ID\n",
    "            topic_id = row[\"Topic ID\"]\n",
    "            post_number = 0\n",
    "\n",
    "            while True:\n",
    "                last_post_number = fetch_and_store_data(f\"{api_url}{topic_id}/\", post_number)\n",
    "\n",
    "                if last_post_number is not None:\n",
    "                    post_number = last_post_number\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    print(\"All data successfully fetched and stored in proposals_post_data.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Users Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# API URL for fetching user data\n",
    "user_api_url = \"https://forum.arbitrum.foundation/c/proposals/4.json\"\n",
    "# Specify CSV file path for user data\n",
    "user_csv_file_path = \"general_users_data.csv\"\n",
    "\n",
    "def fetch_users(api_url, page):\n",
    "    # Make request to fetch user data for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract user data from the response\n",
    "    users = data[\"users\"] if \"users\" in data else []\n",
    "    return users\n",
    "\n",
    "# Set to store processed user IDs\n",
    "processed_user_ids = set()\n",
    "\n",
    "# Write user data to CSV file\n",
    "with open(user_csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as user_csv_file:\n",
    "    # Create CSV writer for user data\n",
    "    user_csv_writer = csv.writer(user_csv_file)\n",
    "\n",
    "    # Write user data header\n",
    "    user_header = [\"User Id\", \"Username\", \"Name\", \"Avtar Template\", \"Moderator\", \"Trust Level\"]\n",
    "\n",
    "    user_csv_writer.writerow(user_header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write user data until there are no more pages\n",
    "    while True:\n",
    "        # Fetch user data for the current page\n",
    "        users = fetch_users(user_api_url, page_num)\n",
    "\n",
    "        # Break the loop if no users are returned\n",
    "        if not users:\n",
    "            break\n",
    "\n",
    "        # Write user details, avoiding duplicates\n",
    "        for user_data in users:\n",
    "            user_id = user_data[\"id\"]\n",
    "\n",
    "            # Check if the user ID has already been processed\n",
    "            if user_id not in processed_user_ids:\n",
    "                # Add the user ID to the set of processed IDs\n",
    "                processed_user_ids.add(user_id)\n",
    "\n",
    "                # Initialize user data list with default values\n",
    "                user_row = [\n",
    "                    user_id,\n",
    "                    user_data[\"username\"],\n",
    "                    user_data[\"name\"],\n",
    "                    user_data[\"avatar_template\"],\n",
    "                    user_data.get(\"moderator\", False),\n",
    "                    user_data[\"trust_level\"],\n",
    "                ]\n",
    "\n",
    "                user_csv_writer.writerow(user_row)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"User data has been written to {user_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Topics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://forum.arbitrum.foundation/c/proposals/4.json\"\n",
    "\n",
    "# Specify CSV file path\n",
    "csv_file_path = \"general_topics_data.csv\"\n",
    "\n",
    "def fetch_topics(api_url, page):\n",
    "    # Make request for the specified page\n",
    "    response = requests.get(f\"{api_url}?page={page}\")\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract topics from the current page\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    return topics\n",
    "\n",
    "# Function to format date and time\n",
    "def format_datetime(datetime_str):\n",
    "    if datetime_str:\n",
    "        # Convert the string to a datetime object\n",
    "        dt_object = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        # Format the datetime object as a string in the desired format\n",
    "        return dt_object.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    # Create CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write header\n",
    "    header = [\"Topic ID\", \"Title\", \"Fancy Title\", \"Slug\", \"Posts Count\", \"Reply Count\", \"Highest Post Number\",\n",
    "              \"Image URL\", \"Created At\", \"Last Posted At\", \"Views\", \"Like Count\", \"Pinned\", \"Unpinned\", \"Closed\", \"Visible\",\n",
    "              \"Tags\", \"Last Poster Username\", \"Category ID\", \"posters\", \"Original Poster ID\"]\n",
    "\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Initialize page number\n",
    "    page_num = 0\n",
    "\n",
    "    # Fetch and write topics until there are no more topics\n",
    "    while True:\n",
    "        topics = fetch_topics(api_url, page_num)\n",
    "\n",
    "        # Break the loop if no topics are returned\n",
    "        if not topics:\n",
    "            break\n",
    "\n",
    "        # Write topic details\n",
    "        for topic in topics:\n",
    "            # Extract the user IDs of the posters, excluding the original poster\n",
    "            user_ids_posters = [poster[\"user_id\"] for poster in topic[\"posters\"] if poster[\"user_id\"] != topic[\"posters\"][0][\"user_id\"]]\n",
    "\n",
    "            # Initialize data list with default values\n",
    "            data = [\n",
    "                topic[\"id\"],\n",
    "                topic[\"title\"],\n",
    "                topic[\"fancy_title\"],\n",
    "                topic[\"slug\"],\n",
    "                topic[\"posts_count\"],\n",
    "                topic[\"reply_count\"],\n",
    "                topic[\"highest_post_number\"],\n",
    "                topic[\"image_url\"],\n",
    "                format_datetime(topic[\"created_at\"]),  # Format Created At\n",
    "                format_datetime(topic[\"last_posted_at\"]),  # Format Last Posted At\n",
    "                topic[\"views\"],\n",
    "                topic[\"like_count\"],\n",
    "                topic[\"pinned\"],\n",
    "                topic[\"unpinned\"],\n",
    "                topic[\"closed\"],\n",
    "                topic[\"visible\"],\n",
    "                topic[\"tags\"],\n",
    "                topic[\"last_poster_username\"],\n",
    "                topic[\"category_id\"],\n",
    "                topic[\"posters\"],\n",
    "                topic[\"posters\"][0][\"user_id\"] if topic[\"posters\"] else None,\n",
    "            ]\n",
    "\n",
    "            csv_writer.writerow(data)\n",
    "\n",
    "        # Move to the next page\n",
    "        page_num += 1\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Posts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch and store data from a specific API endpoint\n",
    "def fetch_and_store_data(api_url, post_number):\n",
    "    url = f\"{api_url}{post_number}.json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"post_stream\" in data and \"posts\" in data[\"post_stream\"]:\n",
    "            posts = data[\"post_stream\"][\"posts\"]\n",
    "\n",
    "            # Remove duplicates based on post_number\n",
    "            posts = [post for post in posts if post[\"post_number\"] > post_number]\n",
    "\n",
    "            if posts:\n",
    "                # Write data to CSV file\n",
    "                with open(\"general_posts_data.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "                    fieldnames = [\"Topic ID\", \"Username\", \"Post Created At\", \"Post Description\", \"Post Number\"]\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "                    # Write headers if the file is empty\n",
    "                    if csv_file.tell() == 0:\n",
    "                        writer.writeheader()\n",
    "\n",
    "                    # Write posts data\n",
    "                    for post in posts:\n",
    "                        # Use BeautifulSoup to extract text from HTML\n",
    "                        soup = BeautifulSoup(post[\"cooked\"], \"html.parser\")\n",
    "                        cleaned_text = soup.get_text()\n",
    "\n",
    "                        # Format date\n",
    "                        formatted_date = datetime.strptime(post[\"created_at\"], \"%Y-%m-%dT%H:%M:%S.%fZ\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                        writer.writerow({\n",
    "                            \"Topic ID\": post[\"topic_id\"],\n",
    "                            \"Username\": post[\"username\"],\n",
    "                            \"Post Created At\": formatted_date,\n",
    "                            \"Post Description\": cleaned_text,\n",
    "                            \"Post Number\": post[\"post_number\"]\n",
    "                        })\n",
    "\n",
    "                print(f\"Data from {url} successfully fetched and stored.\")\n",
    "                return posts[-1][\"post_number\"] + 1  # Increment post_number based on the last post number\n",
    "            else:\n",
    "                print(\"No new posts found.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Error in response data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error fetching data from {url}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Specify API URL\n",
    "    api_url = \"https://forum.arbitrum.foundation/t/\"\n",
    "\n",
    "    # Read Topic IDs from the CSV file generated by ProposalsTopicSummary.py\n",
    "    with open(\"general_topics_data.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            # Fetch and store data for each Topic ID\n",
    "            topic_id = row[\"Topic ID\"]\n",
    "            post_number = 0\n",
    "\n",
    "            while True:\n",
    "                last_post_number = fetch_and_store_data(f\"{api_url}{topic_id}/\", post_number)\n",
    "\n",
    "                if last_post_number is not None:\n",
    "                    post_number = last_post_number\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    print(\"All data successfully fetched and stored in proposals_post_data.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forum Users Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the directory path from the environment variable\n",
    "directory = os.getenv('CSV_DIRECTORY')\n",
    "\n",
    "# Function to combine CSV files in a directory\n",
    "def combine_csv_files(directory, output_filename):\n",
    "    user_files = [file for file in os.listdir(directory) if file.endswith('_users_data.csv')]\n",
    "    combined_df = pd.concat((pd.read_csv(os.path.join(directory, file)) for file in user_files))\n",
    "    combined_df.drop_duplicates(inplace=True)  # Remove duplicate rows\n",
    "    combined_df.to_csv(output_filename, index=False)\n",
    "\n",
    "# Combine users CSV files\n",
    "combine_csv_files(directory, 'forum_users_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forum Topics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the directory path from the environment variable\n",
    "directory = os.getenv('CSV_DIRECTORY')\n",
    "\n",
    "# Function to combine CSV files in a directory\n",
    "def combine_csv_files(directory, output_filename):\n",
    "    user_files = [file for file in os.listdir(directory) if file.endswith('_topics_data.csv')]\n",
    "    combined_df = pd.concat((pd.read_csv(os.path.join(directory, file)) for file in user_files))\n",
    "    combined_df.to_csv(output_filename, index=False)\n",
    "\n",
    "# Combine topics CSV files\n",
    "combine_csv_files(directory, 'forum_topics_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forum Posts Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the directory path from the environment variable\n",
    "directory = os.getenv('CSV_DIRECTORY')\n",
    "\n",
    "# Function to combine CSV files in a directory\n",
    "def combine_csv_files(directory, output_filename):\n",
    "    user_files = [file for file in os.listdir(directory) if file.endswith('_posts_data.csv')]\n",
    "    combined_df = pd.concat((pd.read_csv(os.path.join(directory, file)) for file in user_files))\n",
    "    combined_df.to_csv(output_filename, index=False)\n",
    "\n",
    "# Combine posts CSV files\n",
    "combine_csv_files(directory, 'forum_posts_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
